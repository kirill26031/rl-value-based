{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecdaea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "# from collections import deque\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba009963",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'FrankaKitchen-v1'\n",
    "task = 'kettle'\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cf22ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "granularity = 4\n",
    "flat_dim = 59 + 7\n",
    "action_space_size = granularity*9+1\n",
    "transform_action_space = np.linspace(start=-1.0, stop=1.0, num=granularity)\n",
    "\n",
    "def transform_action_from_int(action: int):\n",
    "    if isinstance(action, list) :\n",
    "        return action\n",
    "    array_action = np.zeros(9)\n",
    "    if action > action_space_size-1 or action < 0:\n",
    "        raise AssertionError(\"transform_action_from_int\")\n",
    "    if action == action_space_size-1:\n",
    "        return array_action\n",
    "    which_action = action % 9\n",
    "    singular_action = transform_action_space[action // 9]\n",
    "    array_action[which_action] = singular_action\n",
    "    return array_action\n",
    "\n",
    "def transform_action_to_int(action) -> int:\n",
    "    if isinstance(action, np.int64) :\n",
    "        return action\n",
    "    if sum(action) == 0.0:\n",
    "        return action_space_size-1\n",
    "    for i in range(9):\n",
    "        if action[i] != 0.0:\n",
    "            value = action[i]\n",
    "            closest_quantized = min(transform_action_space, key=lambda x:abs(x - value))\n",
    "            closest_quantized_index = -1\n",
    "            for j in range(len(transform_action_space)):\n",
    "                if transform_action_space[j] == closest_quantized:\n",
    "                    closest_quantized_index = j\n",
    "            if closest_quantized_index == -1: raise AssertionError(\"transform_action_to_int index is -1\")\n",
    "            return i + 9 * closest_quantized_index\n",
    "    raise AssertionError(\"transform_action_to_int shouldn't be here\")\n",
    "\n",
    "def flatten_observation(observation):\n",
    "    if not isinstance(observation, dict):\n",
    "        return observation\n",
    "    achieved = observation['achieved_goal'][task].astype(np.float32)\n",
    "    obs = observation['observation'].astype(np.float32)\n",
    "\n",
    "    flat_obs = np.concatenate([achieved, obs], dtype=np.float32)\n",
    "    return flat_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b9b4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_reward(observation):\n",
    "    achieved = observation['achieved_goal'][task][0:4]\n",
    "    desired = observation['desired_goal'][task][0:4]\n",
    "    res = 1.0 - np.linalg.norm(achieved - desired)\n",
    "    assert res <= 1.0 and res >= 0.0, \"Reward out of range!\"\n",
    "    return res   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8178691",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_low = np.full((flat_dim,), -1e10, dtype=np.float32)\n",
    "obs_high = np.full((flat_dim,), 1e10, dtype=np.float32)\n",
    "\n",
    "class FlattenDictWrapper(gym.Wrapper):    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.keys = env.observation_space.spaces.keys()\n",
    "        self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, shape=(flat_dim,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Discrete(n=action_space_size)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return flatten_observation(observation)\n",
    "    \n",
    "    def action(self, action):\n",
    "        return transform_action_to_int(action)\n",
    "    \n",
    "    def step(self, action):\n",
    "        transformed_action = transform_action_to_int(action)\n",
    "        obs, reward, terminated, truncated, info = self.env.step(transformed_action)\n",
    "        if reward == 0.0:\n",
    "            reward = custom_reward(obs)\n",
    "        obs = flatten_observation(obs)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        return flatten_observation(obs)\n",
    "    \n",
    "    \n",
    "def make_env():\n",
    "    env = gym.make(env_id, render_mode=None, tasks_to_complete=[task])  # Or your actual task\n",
    "    env = FlattenDictWrapper(env)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_envs = 16\n",
    "env = DummyVecEnv([make_env]*n_training_envs)\n",
    "eval_env = DummyVecEnv([make_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b078560",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timesteps = 50000\n",
    "exploration_fraction=0.95\n",
    "run_name = f\"dqn_{granularity}p_{max_timesteps}_reward_shaping_{int(exploration_fraction*100)}_\"+task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9700d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\"MlpPolicy\", env, device=device, exploration_fraction=exploration_fraction, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03089eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DQN.load(\"dqn_3_10000_\"+task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_log_dir = os.path.join(\"eval_logs\", run_name)\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=eval_log_dir,\n",
    "                              log_path=eval_log_dir, eval_freq=max(500 // n_training_envs, 1),\n",
    "                              n_eval_episodes=5, deterministic=True,\n",
    "                              render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f5010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=496, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=992, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=1488, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=1984, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=2480, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=2976, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=3472, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=3968, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=4464, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=4960, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=5456, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=5952, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=6448, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=6944, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=7440, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=7936, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=8432, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=8928, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=9424, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=9920, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=10416, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=10912, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=11408, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=11904, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=12400, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=12896, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=13392, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=13888, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=14384, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=14880, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=15376, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=15872, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=16368, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=16864, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=17360, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=17856, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=18352, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=18848, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=19344, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=19840, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=20336, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=20832, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=21328, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=21824, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=22320, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=22816, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=23312, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=23808, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=24304, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=24800, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=25296, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=25792, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=26288, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=26784, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=27280, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=27776, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=28272, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=28768, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=29264, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=29760, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=30256, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=30752, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=31248, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=31744, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=32240, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=32736, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=33232, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=33728, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=34224, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=34720, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=35216, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=35712, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=36208, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=36704, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=37200, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=37696, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=38192, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=38688, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=39184, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=39680, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=40176, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=40672, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=41168, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=41664, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=42160, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=42656, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=43152, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=43648, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=44144, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=44640, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=45136, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=45632, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=46128, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=46624, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=47120, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=47616, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=48112, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=48608, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=49104, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=49600, episode_reward=166.28 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f65a9ce03d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.learn(total_timesteps=max_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95743a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70579843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 166.28406681999144\n",
      "Episode reward: 166.2840668199915\n",
      "Episode reward: 166.28406681999152\n",
      "Episode reward: 166.28406681999152\n",
      "Episode reward: 166.2840668199915\n",
      "Episode reward: 166.28406681999152\n",
      "Episode reward: 166.28406681999144\n",
      "Episode reward: 166.28406681999144\n",
      "Episode reward: 166.28406681999147\n",
      "Episode reward: 166.28406681999144\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\tenv_eval = make_env()\n",
    "\tobs, _ = env_eval.reset()\n",
    "\tdone = False\n",
    "\tep_reward = 0\n",
    "\n",
    "\twhile not done:\n",
    "\t\taction, _ = model.predict(obs, deterministic=True)\n",
    "\t\tobs, reward, terminated, truncated, _ = env_eval.step(transform_action_from_int(action))\n",
    "\t\tobs = flatten_observation(obs)\n",
    "\t\tdone = terminated or truncated\n",
    "\t\tep_reward += reward\n",
    "\tprint(f\"Episode reward: {ep_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df0f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'observation': array([ 1.47801565e-01, -1.76829107e+00,  1.84395217e+00, -2.47610622e+00,\n",
       "          2.60691996e-01,  7.12663739e-01,  1.59498559e+00,  4.87271619e-02,\n",
       "          3.67021062e-02, -2.53472528e-04, -2.65877959e-04,  2.26002676e-04,\n",
       "         -3.65829334e-04,  5.72075854e-04,  6.87336170e-04,  8.14281501e-04,\n",
       "         -1.63403919e-05, -1.42713245e-04, -2.67131394e-04, -5.12181348e-05,\n",
       "          3.13329051e-05, -4.53444766e-05, -3.82073543e-06, -4.20677973e-05,\n",
       "          6.28998687e-05,  4.04362722e-05,  4.62748053e-04, -2.26011323e-04,\n",
       "         -4.67009093e-04, -6.44076225e-03, -1.79233727e-03,  1.05146686e-03,\n",
       "         -2.69397033e-01,  3.50382421e-01,  1.61944820e+00,  9.99968903e-01,\n",
       "          4.00662752e-03, -6.59483102e-03, -2.88975688e-04, -1.05641845e-06,\n",
       "         -1.57191980e-06,  1.32664513e-06,  1.38601982e-06, -1.43032028e-06,\n",
       "          1.43688572e-06, -8.37722653e-08,  1.46978715e-06, -3.77669880e-07,\n",
       "          1.92717639e-06, -1.84364257e-06, -1.22620412e-05,  1.56809964e-05,\n",
       "          1.74945581e-05,  1.65521348e-06, -3.56590283e-07, -1.54225553e-06,\n",
       "         -3.04471028e-05, -3.45525635e-05, -4.21457035e-05]),\n",
       "  'achieved_goal': {'kettle': array([-2.69397440e-01,  3.50383255e-01,  1.61944683e+00,  9.99970159e-01,\n",
       "           4.03883905e-03, -6.58004743e-03, -2.66621172e-04])},\n",
       "  'desired_goal': {'kettle': array([-0.23,  0.75,  1.62,  0.99,  0.  ,  0.  , -0.06])}},\n",
       " {'tasks_to_complete': ['kettle'],\n",
       "  'episode_task_completions': [],\n",
       "  'step_task_completions': []})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env_ = gym.make(env_id, render_mode=None, tasks_to_complete=[task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env_.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276704e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': array([ 1.49064696e-01, -1.76751037e+00,  1.84415087e+00, -2.47686538e+00,\n",
       "         2.59389739e-01,  7.12103377e-01,  1.59467207e+00,  4.76014193e-02,\n",
       "         3.72559511e-02,  1.24158133e-04, -4.72168496e-04,  2.44411661e-04,\n",
       "        -7.30645572e-05, -9.94192991e-04,  1.76777846e-04, -4.32781443e-04,\n",
       "         2.40148757e-04,  5.28851150e-04, -2.40551674e-04, -5.30422618e-05,\n",
       "         3.29306957e-05, -4.48706136e-05, -3.81012541e-06, -4.22421163e-05,\n",
       "         6.28081484e-05,  4.02719850e-05,  4.62976087e-04, -2.29308999e-04,\n",
       "        -4.66343836e-04, -6.43987546e-03, -1.72089090e-03,  1.05306444e-03,\n",
       "        -2.69415644e-01,  3.50383656e-01,  1.61944537e+00,  9.99971732e-01,\n",
       "         3.99291977e-03, -6.57423291e-03, -3.15833682e-04, -1.10305015e-06,\n",
       "         1.05751294e-06, -1.67263545e-06,  1.35567182e-06, -1.21150897e-06,\n",
       "        -2.21962015e-07,  2.02304157e-06,  8.22931005e-07, -1.77298674e-06,\n",
       "         1.34866114e-06,  3.18721696e-07,  2.37975197e-05,  2.23025188e-05,\n",
       "        -1.75580747e-05, -1.75516163e-06,  2.28247160e-07, -1.87947467e-06,\n",
       "         3.39149117e-05,  8.58951430e-06, -3.89317115e-05]),\n",
       " 'achieved_goal': {'kettle': array([-2.69397440e-01,  3.50383255e-01,  1.61944683e+00,  9.99970159e-01,\n",
       "          4.03883905e-03, -6.58004743e-03, -2.66621172e-04])},\n",
       " 'desired_goal': {'kettle': array([-0.23,  0.75,  1.62,  0.99,  0.  ,  0.  , -0.06])}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95710db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = state[0]\n",
    "desired_xyz = sample['desired_goal']['kettle'][0:3]\n",
    "obs_xyz = sample['observation'][32:35]\n",
    "distance_for_kettle = desired_xyz - obs_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6365dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03941564, 0.39961634, 0.00055463])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_for_kettle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
