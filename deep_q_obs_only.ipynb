{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecdaea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "# from collections import deque\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba009963",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'FrankaKitchen-v1'\n",
    "task = 'kettle'\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cf22ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "granularity = 4\n",
    "flat_dim = 59 + 7\n",
    "action_space_size = granularity*9+1\n",
    "transform_action_space = np.linspace(start=-1.0, stop=1.0, num=granularity)\n",
    "\n",
    "def transform_action_from_int(action: int):\n",
    "    if isinstance(action, list) :\n",
    "        return action\n",
    "    array_action = np.zeros(9)\n",
    "    if action > action_space_size-1 or action < 0:\n",
    "        raise AssertionError(\"transform_action_from_int\")\n",
    "    if action == action_space_size-1:\n",
    "        return array_action\n",
    "    which_action = action % 9\n",
    "    singular_action = transform_action_space[action // 9]\n",
    "    array_action[which_action] = singular_action\n",
    "    return array_action\n",
    "\n",
    "def transform_action_to_int(action) -> int:\n",
    "    if isinstance(action, np.int64) :\n",
    "        return action\n",
    "    if sum(action) == 0.0:\n",
    "        return action_space_size-1\n",
    "    for i in range(9):\n",
    "        if action[i] != 0.0:\n",
    "            value = action[i]\n",
    "            closest_quantized = min(transform_action_space, key=lambda x:abs(x - value))\n",
    "            closest_quantized_index = -1\n",
    "            for j in range(len(transform_action_space)):\n",
    "                if transform_action_space[j] == closest_quantized:\n",
    "                    closest_quantized_index = j\n",
    "            if closest_quantized_index == -1: raise AssertionError(\"transform_action_to_int index is -1\")\n",
    "            return i + 9 * closest_quantized_index\n",
    "    raise AssertionError(\"transform_action_to_int shouldn't be here\")\n",
    "\n",
    "def flatten_observation(observation):\n",
    "    if not isinstance(observation, dict):\n",
    "        return observation\n",
    "    achieved = observation['achieved_goal'][task].astype(np.float32)\n",
    "    obs = observation['observation'].astype(np.float32)\n",
    "\n",
    "    flat_obs = np.concatenate([achieved, obs], dtype=np.float32)\n",
    "    return flat_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b9b4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_reward(observation):\n",
    "    achieved = observation['achieved_goal'][task][0:4]\n",
    "    desired = observation['desired_goal'][task][0:4]\n",
    "    res = 1.0 - np.linalg.norm(achieved - desired)\n",
    "    assert res <= 1.0 and res >= 0.0, \"Reward out of range!\"\n",
    "    return res   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8178691",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_low = np.full((flat_dim,), -1e10, dtype=np.float32)\n",
    "obs_high = np.full((flat_dim,), 1e10, dtype=np.float32)\n",
    "\n",
    "class FlattenDictWrapper(gym.Wrapper):    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.keys = env.observation_space.spaces.keys()\n",
    "        self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, shape=(flat_dim,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Discrete(n=action_space_size)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return flatten_observation(observation)\n",
    "    \n",
    "    def action(self, action):\n",
    "        return transform_action_to_int(action)\n",
    "    \n",
    "    def step(self, action):\n",
    "        transformed_action = transform_action_to_int(action)\n",
    "        obs, reward, terminated, truncated, info = self.env.step(transformed_action)\n",
    "        if reward == 0.0:\n",
    "            reward = custom_reward(obs)\n",
    "        obs = flatten_observation(obs)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        return flatten_observation(obs)\n",
    "    \n",
    "    \n",
    "def make_env():\n",
    "    env = gym.make(env_id, render_mode=None, tasks_to_complete=[task])  # Or your actual task\n",
    "    env = FlattenDictWrapper(env)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bcb1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_envs = 16\n",
    "env = DummyVecEnv([make_env]*n_training_envs)\n",
    "eval_env = DummyVecEnv([make_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b078560",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timesteps = 50000\n",
    "exploration_fraction=0.95\n",
    "run_name = f\"dqn_{granularity}p_{max_timesteps}_reward_shaping_{int(exploration_fraction*100)}_\"+task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9700d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\"MlpPolicy\", env, device=device, exploration_fraction=exploration_fraction, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03089eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DQN.load(\"dqn_3_10000_\"+task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74b2263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_log_dir = os.path.join(\"eval_logs\", run_name)\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=eval_log_dir,\n",
    "                              log_path=eval_log_dir, eval_freq=max(500 // n_training_envs, 1),\n",
    "                              n_eval_episodes=5, deterministic=True,\n",
    "                              render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da9f5010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=496, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=992, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=1488, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=1984, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=2480, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=2976, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=3472, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=3968, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=4464, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=4960, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=5456, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=5952, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=6448, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=6944, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=7440, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=7936, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=8432, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=8928, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=9424, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=9920, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=10416, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=10912, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=11408, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=11904, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=12400, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=12896, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=13392, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=13888, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=14384, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=14880, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=15376, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=15872, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=16368, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=16864, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=17360, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=17856, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=18352, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=18848, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=19344, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=19840, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=20336, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=20832, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=21328, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=21824, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=22320, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=22816, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=23312, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=23808, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=24304, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=24800, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=25296, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=25792, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=26288, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=26784, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=27280, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=27776, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=28272, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=28768, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=29264, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=29760, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=30256, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=30752, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=31248, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=31744, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=32240, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=32736, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=33232, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=33728, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=34224, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=34720, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=35216, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=35712, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=36208, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=36704, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=37200, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=37696, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=38192, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=38688, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=39184, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=39680, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=40176, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=40672, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=41168, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=41664, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=42160, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=42656, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=43152, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=43648, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=44144, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=44640, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=45136, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=45632, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=46128, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=46624, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=47120, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=47616, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=48112, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=48608, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=49104, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=49600, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fd05f5834f0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.learn(total_timesteps=max_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c95743a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70579843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 167.51982922244304\n",
      "Episode reward: 167.51982922244306\n",
      "Episode reward: 167.5198292224431\n",
      "Episode reward: 167.5198292224431\n",
      "Episode reward: 167.5198292224431\n",
      "Episode reward: 167.51982922244306\n",
      "Episode reward: 167.51982922244306\n",
      "Episode reward: 167.51982922244304\n",
      "Episode reward: 167.5198292224431\n",
      "Episode reward: 167.5198292224431\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\tenv_eval = make_env()\n",
    "\tobs, _ = env_eval.reset()\n",
    "\tdone = False\n",
    "\tep_reward = 0\n",
    "\n",
    "\twhile not done:\n",
    "\t\taction, _ = model.predict(obs, deterministic=True)\n",
    "\t\tobs, reward, terminated, truncated, _ = env_eval.step(transform_action_from_int(action))\n",
    "\t\tobs = flatten_observation(obs)\n",
    "\t\tdone = terminated or truncated\n",
    "\t\tep_reward += reward\n",
    "\tprint(f\"Episode reward: {ep_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22df0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_ = gym.make(env_id, render_mode=None, tasks_to_complete=[task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe2a170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env_.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "276704e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': array([ 1.48711058e-01, -1.76751755e+00,  1.84306545e+00, -2.47715988e+00,\n",
       "         2.59674953e-01,  7.13106755e-01,  1.59510009e+00,  4.73113960e-02,\n",
       "         3.76087782e-02, -4.77725274e-04,  6.46840369e-04,  2.66336739e-05,\n",
       "         8.08748015e-04,  5.60853916e-04, -8.05000536e-04, -9.80934344e-04,\n",
       "         5.83153126e-04, -9.08293820e-04, -2.19807747e-04, -5.05184171e-05,\n",
       "         3.32462059e-05, -4.50442273e-05, -3.94169622e-06, -4.24391463e-05,\n",
       "         6.26077947e-05,  4.03237042e-05,  4.62669503e-04, -2.26772798e-04,\n",
       "        -4.66765698e-04, -6.44101652e-03, -1.81471533e-03,  1.12579119e-03,\n",
       "        -2.69440620e-01,  3.50381496e-01,  1.61944719e+00,  9.99972244e-01,\n",
       "         4.08331116e-03, -6.59475048e-03, -2.48649692e-04,  1.09479091e-06,\n",
       "        -7.90423333e-07,  1.93835756e-06,  6.32993734e-07, -9.95662358e-07,\n",
       "        -3.71688496e-07,  2.06505846e-06,  8.71959368e-07, -8.29948416e-07,\n",
       "         1.67522481e-06,  1.83082687e-07, -4.50304545e-06,  1.92959564e-05,\n",
       "        -2.54205210e-06, -2.10411861e-06, -1.55997690e-06, -2.08683934e-06,\n",
       "         2.43791907e-05,  2.36280716e-05,  7.32177017e-06]),\n",
       " 'achieved_goal': {'kettle': array([-2.69397440e-01,  3.50383255e-01,  1.61944683e+00,  9.99970159e-01,\n",
       "          4.03883905e-03, -6.58004743e-03, -2.66621172e-04])},\n",
       " 'desired_goal': {'kettle': array([-0.23,  0.75,  1.62,  0.99,  0.  ,  0.  , -0.06])}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e95710db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = state[0]\n",
    "desired_xyz = sample['desired_goal']['kettle'][0:3]\n",
    "obs_xyz = sample['observation'][32:35]\n",
    "distance_for_kettle = desired_xyz - obs_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df6365dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03944062, 0.3996185 , 0.00055281])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_for_kettle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
