{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41a0c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# from collections import deque\n",
    "import numpy as np\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8a571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'FrankaKitchen-v1'\n",
    "task = 'kettle'\n",
    "gym.register_envs(gymnasium_robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2769def",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2c4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253230ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_dim = 73\n",
    "obs_low = np.full((flat_dim,), -1e10, dtype=np.float32)\n",
    "obs_high = np.full((flat_dim,), 1e10, dtype=np.float32)\n",
    "\n",
    "class FlattenDictWrapper(gym.ObservationWrapper):    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.keys = env.observation_space.spaces.keys()\n",
    "        self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, shape=(flat_dim,), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        achieved = observation['achieved_goal'][task].astype(np.float32)\n",
    "        desired = observation['desired_goal'][task].astype(np.float32)\n",
    "        obs = observation['observation'].astype(np.float32)\n",
    "\n",
    "        flat_obs = np.concatenate([achieved, desired, obs], dtype=np.float32)\n",
    "        return flat_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93560185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make(env_id, render_mode=None, tasks_to_complete=[task])  # Or your actual task\n",
    "    env = FlattenDictWrapper(env)\n",
    "    return env\n",
    "\n",
    "n_training_envs = 64\n",
    "env = DummyVecEnv([make_env]*n_training_envs)\n",
    "eval_env = DummyVecEnv([make_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a85f96c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timesteps = 250000\n",
    "run_name = f\"ddpg_{max_timesteps}_\"+task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d44c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_log_dir = os.path.join(\"eval_logs\", run_name)\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=eval_log_dir,\n",
    "                              log_path=eval_log_dir, eval_freq=max(500 // n_training_envs, 1),\n",
    "                              n_eval_episodes=5, deterministic=True,\n",
    "                              render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7db8869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.14GB > 1.35GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The noise objects for DDPG\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1, device=device, buffer_size=5000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8f00759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=448, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 448      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.329   |\n",
      "|    critic_loss     | 0.0398   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5        |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=896, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 896      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.149   |\n",
      "|    critic_loss     | 0.00768  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1344, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1344     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.226   |\n",
      "|    critic_loss     | 0.00161  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1792, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1792     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.205   |\n",
      "|    critic_loss     | 0.000448 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 26       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2240, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2240     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.205   |\n",
      "|    critic_loss     | 0.00019  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 33       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2688, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2688     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.226   |\n",
      "|    critic_loss     | 7.64e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 40       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3136, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3136     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.215   |\n",
      "|    critic_loss     | 0.000122 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 47       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3584, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3584     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.227   |\n",
      "|    critic_loss     | 6.17e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 54       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4032, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4032     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.232   |\n",
      "|    critic_loss     | 6.73e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 61       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4480, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4480     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.239   |\n",
      "|    critic_loss     | 2.58e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 68       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4928, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4928     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.238   |\n",
      "|    critic_loss     | 3.59e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 75       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5376, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5376     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.24    |\n",
      "|    critic_loss     | 1.66e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 82       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5824, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5824     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.249   |\n",
      "|    critic_loss     | 1.63e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 89       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6272, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6272     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.249   |\n",
      "|    critic_loss     | 1.39e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 96       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6720, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6720     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.252   |\n",
      "|    critic_loss     | 9.35e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 103      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7168, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7168     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 1.02e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 110      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7616, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7616     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 9.01e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 117      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8064, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8064     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 1.04e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 124      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8512, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8512     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.262   |\n",
      "|    critic_loss     | 8.76e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 131      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8960, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8960     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.262   |\n",
      "|    critic_loss     | 6.35e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 138      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9408, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9408     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.261   |\n",
      "|    critic_loss     | 8.41e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 145      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9856, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9856     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.263   |\n",
      "|    critic_loss     | 7.38e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 152      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10304, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10304    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.265   |\n",
      "|    critic_loss     | 4.17e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 159      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10752, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10752    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.261   |\n",
      "|    critic_loss     | 7.92e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 166      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11200, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.265   |\n",
      "|    critic_loss     | 7.64e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 173      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11648, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11648    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.264   |\n",
      "|    critic_loss     | 4.13e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 180      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12096, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12096    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.263   |\n",
      "|    critic_loss     | 4.65e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 187      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12544, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12544    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.262   |\n",
      "|    critic_loss     | 3.75e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 194      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12992, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12992    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.262   |\n",
      "|    critic_loss     | 2.72e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 201      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13440, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13440    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.262   |\n",
      "|    critic_loss     | 2.92e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 208      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13888, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13888    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 2.87e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 215      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14336, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14336    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.259   |\n",
      "|    critic_loss     | 4.38e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 222      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14784, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14784    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 5.52e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 229      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15232, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15232    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 1.87e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 236      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15680, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15680    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.259   |\n",
      "|    critic_loss     | 4.61e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 243      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16128, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16128    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 3.24e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 250      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16576, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16576    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 4.15e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 257      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17024, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17024    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 8.57e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 264      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17472, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17472    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 6.62e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 271      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17920, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17920    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 2.73e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 278      |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 10    |\n",
      "|    fps             | 15    |\n",
      "|    time_elapsed    | 1134  |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 20    |\n",
      "|    fps             | 15    |\n",
      "|    time_elapsed    | 1134  |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 30    |\n",
      "|    fps             | 15    |\n",
      "|    time_elapsed    | 1134  |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 40    |\n",
      "|    fps             | 15    |\n",
      "|    time_elapsed    | 1134  |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 50    |\n",
      "|    fps             | 15    |\n",
      "|    time_elapsed    | 1134  |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 60    |\n",
      "|    fps             | 15    |\n",
      "|    time_elapsed    | 1134  |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18368, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18368    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 3.68e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 285      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18816, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18816    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 3.04e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 292      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19264, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19264    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 2.22e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 299      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19712, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19712    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 2.37e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 306      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20160, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20160    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 1.67e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20608, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20608    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 1.83e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 320      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21056, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21056    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 1.63e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 327      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21504, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21504    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 1.54e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 334      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21952, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21952    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 1.26e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 341      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22400, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 1.11e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 348      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22848, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22848    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 2.29e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 355      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23296, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23296    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 2.04e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 362      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23744, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23744    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 2.63e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 369      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24192, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24192    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 2.34e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 376      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24640, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24640    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 1.68e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 383      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25088, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25088    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 2.13e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 390      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25536, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25536    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 1.46e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 397      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25984, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25984    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 1.29e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 404      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26432, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26432    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 1.44e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 411      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26880, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26880    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 8.41e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 418      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27328, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27328    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 1.5e-06  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 425      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27776, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27776    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.261   |\n",
      "|    critic_loss     | 1.41e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 432      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28224, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28224    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 3.09e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 439      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28672, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28672    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 1.56e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 446      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29120, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29120    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 2.8e-06  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 453      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29568, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29568    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 1.79e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 460      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30016, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30016    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 3.23e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 467      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30464, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30464    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 3.62e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 474      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30912, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30912    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 2.43e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 481      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31360, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31360    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 1.87e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 488      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31808, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31808    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 2.79e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 495      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32256, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32256    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.261   |\n",
      "|    critic_loss     | 3.7e-06  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 502      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32704, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32704    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 1.1e-06  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 509      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33152, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33152    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 2.35e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 516      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33600, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 1.85e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 523      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34048, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34048    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.262   |\n",
      "|    critic_loss     | 9.87e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 530      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34496, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34496    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 7.49e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 537      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34944, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34944    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 1.69e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 544      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35392, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35392    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 8.43e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 551      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35840, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35840    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 5.61e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 558      |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 70    |\n",
      "|    fps             | 15    |\n",
      "|    time_elapsed    | 2243  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 80    |\n",
      "|    fps             | 15    |\n",
      "|    time_elapsed    | 2243  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 90    |\n",
      "|    fps             | 15    |\n",
      "|    time_elapsed    | 2243  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 100   |\n",
      "|    fps             | 15    |\n",
      "|    time_elapsed    | 2243  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 110   |\n",
      "|    fps             | 15    |\n",
      "|    time_elapsed    | 2243  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 120   |\n",
      "|    fps             | 15    |\n",
      "|    time_elapsed    | 2243  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36288, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 7.8e-07  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 565      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36736, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36736    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 9.79e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 572      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37184, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37184    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 1.16e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 579      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37632, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37632    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 9.57e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 586      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38080, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38080    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 1.05e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 593      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38528, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38528    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 8.94e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 600      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38976, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38976    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 7.06e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 607      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39424, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39424    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 5.86e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 614      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39872, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39872    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 4.44e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 621      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40320, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40320    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 6.12e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 628      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40768, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40768    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 1.21e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 635      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41216, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41216    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 6.64e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 642      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41664, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41664    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 1.14e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 649      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42112, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42112    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 9.69e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 656      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42560, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42560    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 7.49e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 663      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43008, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43008    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 8.25e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 670      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43456, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43456    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 5.57e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 677      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43904, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43904    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 5.62e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 684      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44352, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44352    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 6.21e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 691      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44800, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 6.81e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 698      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45248, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45248    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 5.47e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 705      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45696, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45696    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 8.2e-07  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 712      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46144, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.261   |\n",
      "|    critic_loss     | 2.26e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 719      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46592, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46592    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 9.07e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 726      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47040, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47040    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.26    |\n",
      "|    critic_loss     | 5.44e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 733      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47488, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47488    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 1.57e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 740      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47936, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47936    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 1.41e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 747      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48384, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48384    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 4.97e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 754      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48832, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48832    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 6.08e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 761      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49280, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49280    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.258   |\n",
      "|    critic_loss     | 8.29e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 768      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49728, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49728    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 8.46e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 775      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50176, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50176    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 1.03e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 782      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50624, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50624    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 4.25e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 789      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51072, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51072    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 4.5e-07  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 796      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51520, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51520    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 5.78e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 803      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51968, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51968    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 6.71e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 810      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52416, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52416    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 8.86e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 817      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52864, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52864    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 7.09e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 824      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53312, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53312    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 3.07e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 831      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53760, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53760    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.253   |\n",
      "|    critic_loss     | 3.97e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 838      |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 130   |\n",
      "|    fps             | 16    |\n",
      "|    time_elapsed    | 3326  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 140   |\n",
      "|    fps             | 16    |\n",
      "|    time_elapsed    | 3326  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 150   |\n",
      "|    fps             | 16    |\n",
      "|    time_elapsed    | 3326  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 160   |\n",
      "|    fps             | 16    |\n",
      "|    time_elapsed    | 3326  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 170   |\n",
      "|    fps             | 16    |\n",
      "|    time_elapsed    | 3326  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 180   |\n",
      "|    fps             | 16    |\n",
      "|    time_elapsed    | 3326  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 190   |\n",
      "|    fps             | 16    |\n",
      "|    time_elapsed    | 3329  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54208, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54208    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.253   |\n",
      "|    critic_loss     | 2.54e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 845      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54656, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54656    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 1.09e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 852      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55104, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55104    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.253   |\n",
      "|    critic_loss     | 5.46e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 859      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55552, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55552    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 1.59e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 866      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 4.01e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 873      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56448, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56448    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 1.43e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 880      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56896, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56896    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 4.65e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 887      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57344, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57344    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.253   |\n",
      "|    critic_loss     | 5.78e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 894      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57792, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57792    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 4.91e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 901      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58240, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58240    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 1.91e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 908      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58688, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58688    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.257   |\n",
      "|    critic_loss     | 1.74e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 915      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59136, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59136    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 1.44e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 922      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59584, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59584    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 7.09e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 929      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60032, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60032    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 7.12e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 936      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60480    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 4.07e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 943      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60928, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60928    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 4.04e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 950      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61376, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61376    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.252   |\n",
      "|    critic_loss     | 5.18e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 957      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61824, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61824    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 2.94e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 964      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62272, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62272    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 6e-07    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 971      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62720, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62720    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.253   |\n",
      "|    critic_loss     | 2.5e-06  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 978      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63168, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63168    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 1.41e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 985      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63616, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63616    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.253   |\n",
      "|    critic_loss     | 4.87e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 992      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64064, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64064    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.253   |\n",
      "|    critic_loss     | 3.19e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 999      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64512, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64512    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 1.21e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1006     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64960, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64960    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.242   |\n",
      "|    critic_loss     | 0.00013  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1013     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65408, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65408    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 2.25e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65856, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65856    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.251   |\n",
      "|    critic_loss     | 3.65e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1027     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66304, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66304    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.259   |\n",
      "|    critic_loss     | 2.58e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1034     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66752, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66752    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.251   |\n",
      "|    critic_loss     | 7.63e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1041     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67200, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.252   |\n",
      "|    critic_loss     | 1.82e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67648, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67648    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.256   |\n",
      "|    critic_loss     | 1.36e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1055     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68096, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68096    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.255   |\n",
      "|    critic_loss     | 1.58e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1062     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68544, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68544    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.253   |\n",
      "|    critic_loss     | 6.18e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1069     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68992, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68992    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 8.07e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1076     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69440, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69440    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 8.96e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1083     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69888, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69888    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.252   |\n",
      "|    critic_loss     | 8.86e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70336, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70336    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 3.16e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1097     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70784, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70784    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 3.02e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1104     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71232, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 0        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71232    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.253   |\n",
      "|    critic_loss     | 5.15e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1111     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/ddpg/ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[1;32m    116\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDDPG:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/td3/td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[1;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:568\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    566\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# Only stop training if return value is False, not when it is None.\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RolloutReturn(num_collected_steps \u001b[38;5;241m*\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs, num_collected_episodes, continue_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    571\u001b[0m \u001b[38;5;66;03m# Retrieve reward and episode length if using Monitor wrapper\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:464\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# Reset success rate buffer\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_success_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 464\u001b[0m episode_rewards, episode_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_episode_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_success_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(episode_rewards, \u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:88\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     86\u001b[0m episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((env\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m---> 88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[1;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:557\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/policies.py:365\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 365\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     )\n\u001b[1;32m    371\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/policies.py:276\u001b[0m, in \u001b[0;36mobs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(observation)\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     vectorized_env \u001b[38;5;241m=\u001b[39m is_vectorized_observation(observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/utils.py:488\u001b[0m, in \u001b[0;36mobs_as_tensor\u001b[0;34m(obs, device)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03mMoves the observation to the given device.\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m:return: PyTorch tensor of the observation on a desired device.\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: th\u001b[38;5;241m.\u001b[39mas_tensor(_obs, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m (key, _obs) \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=max_timesteps, log_interval=10, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "616f8110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\tenv_eval = make_env()\n",
    "\tobs, _ = env_eval.reset()\n",
    "\tdone = False\n",
    "\tep_reward = 0\n",
    "\n",
    "\twhile not done:\n",
    "\t\taction, _ = model.predict(obs, deterministic=True)\n",
    "\t\tobs, reward, terminated, truncated, _ = env_eval.step(action)\n",
    "\t\tdone = terminated or truncated\n",
    "\t\tep_reward += reward\n",
    "\tprint(f\"Episode reward: {ep_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Box(-1.0, 1.0, (9,), float64)\n",
      "Model action space: Box(-1.0, 1.0, (9,), float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", env_eval.action_space)\n",
    "print(\"Model action space:\", model.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73d0edd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
