{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ecdaea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "# from collections import deque\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba009963",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'FrankaKitchen-v1'\n",
    "task = 'kettle'\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3cf22ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "granularity = 2\n",
    "transform_action_space = np.linspace(start=-1.0, stop=1.0, num=granularity)\n",
    "\n",
    "def transform_action_from_int(action: int):\n",
    "    if isinstance(action, list) :\n",
    "        return action\n",
    "    array_action = np.zeros(9)\n",
    "    if action > 18 or action < 0:\n",
    "        raise AssertionError(\"transform_action_from_int\")\n",
    "    if action == 18:\n",
    "        return array_action\n",
    "    which_action = action % 9\n",
    "    singular_action = transform_action_space[action // 9]\n",
    "    array_action[which_action] = singular_action\n",
    "    return array_action\n",
    "\n",
    "def transform_action_to_int(action) -> int:\n",
    "    if isinstance(action, np.int64) :\n",
    "        return action\n",
    "    if sum(action) == 0.0:\n",
    "        return 18\n",
    "    for i in range(9):\n",
    "        if action[i] != 0.0:\n",
    "            value = action[i]\n",
    "            closest_quantized = min(transform_action_space, key=lambda x:abs(x - value))\n",
    "            closest_quantized_index = -1\n",
    "            for j in range(len(transform_action_space)):\n",
    "                if transform_action_space[j] == closest_quantized:\n",
    "                    closest_quantized_index = j\n",
    "            if closest_quantized_index == -1: raise AssertionError(\"transform_action_to_int index is -1\")\n",
    "            return i + 9 * closest_quantized_index\n",
    "    raise AssertionError(\"transform_action_to_int shouldn't be here\")\n",
    "\n",
    "def flatten_observation(observation):\n",
    "    if not isinstance(observation, dict):\n",
    "        return observation\n",
    "    achieved = observation['achieved_goal'][task].astype(np.float32)\n",
    "    desired = observation['desired_goal'][task].astype(np.float32)\n",
    "    obs = observation['observation'].astype(np.float32)\n",
    "\n",
    "    flat_obs = np.concatenate([achieved, desired, obs], dtype=np.float32)\n",
    "    return flat_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8178691",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_dim = 73\n",
    "action_space_size = granularity*9+1\n",
    "obs_low = np.full((flat_dim,), -1e10, dtype=np.float32)\n",
    "obs_high = np.full((flat_dim,), 1e10, dtype=np.float32)\n",
    "\n",
    "class FlattenDictWrapper(gym.Wrapper):    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.keys = env.observation_space.spaces.keys()\n",
    "        self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, shape=(flat_dim,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Discrete(n=action_space_size)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return flatten_observation(observation)\n",
    "    \n",
    "    def action(self, action):\n",
    "        return transform_action_to_int(action)\n",
    "    \n",
    "    def step(self, action):\n",
    "        transformed_action = transform_action_to_int(action)\n",
    "        obs, reward, terminated, truncated, info = self.env.step(transformed_action)\n",
    "        obs = flatten_observation(obs)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        return flatten_observation(obs)\n",
    "    \n",
    "    \n",
    "def make_env():\n",
    "    env = gym.make(env_id, render_mode=None, tasks_to_complete=[task])  # Or your actual task\n",
    "    env = FlattenDictWrapper(env)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3bcb1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_envs = 64\n",
    "env = DummyVecEnv([make_env]*n_training_envs)\n",
    "eval_env = DummyVecEnv([make_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5b078560",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timesteps = 250000\n",
    "exploration_fraction=0.95\n",
    "run_name = f\"dqn_{granularity}p_{max_timesteps}_{int(exploration_fraction*100)}_\"+task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d9700d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "model = DQN(\"MlpPolicy\", env, device=device, verbose=1, exploration_fraction=exploration_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03089eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DQN.load(\"dqn_3_10000_\"+task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "74b2263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_log_dir = os.path.join(\"eval_logs\", run_name)\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=eval_log_dir,\n",
    "                              log_path=eval_log_dir, eval_freq=max(500 // n_training_envs, 1),\n",
    "                              n_eval_episodes=5, deterministic=True,\n",
    "                              render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "da9f5010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=448, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.998    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 448      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00986  |\n",
      "|    n_updates        | 1        |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=896, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.997    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 896      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0187   |\n",
      "|    n_updates        | 3        |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1344, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.995    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1344     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0148   |\n",
      "|    n_updates        | 5        |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1792, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.993    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1792     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 6        |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2240, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.991    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2240     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0112   |\n",
      "|    n_updates        | 8        |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2688, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.99     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2688     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0171   |\n",
      "|    n_updates        | 10       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3136, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.988    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 3136     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0138   |\n",
      "|    n_updates        | 12       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3584, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.986    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 3584     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0149   |\n",
      "|    n_updates        | 13       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4032, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.984    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 4032     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0137   |\n",
      "|    n_updates        | 15       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4480, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.982    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 4480     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0114   |\n",
      "|    n_updates        | 17       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4928, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.981    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 4928     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0141   |\n",
      "|    n_updates        | 19       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5376, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.979    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5376     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0163   |\n",
      "|    n_updates        | 20       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5824, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.977    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5824     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0123   |\n",
      "|    n_updates        | 22       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6272, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.975    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 6272     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0116   |\n",
      "|    n_updates        | 24       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6720, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.973    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 6720     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0125   |\n",
      "|    n_updates        | 26       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7168, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.972    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 7168     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0121   |\n",
      "|    n_updates        | 27       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7616, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.97     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 7616     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0133   |\n",
      "|    n_updates        | 29       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8064, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.968    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 8064     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0128   |\n",
      "|    n_updates        | 31       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8512, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.966    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 8512     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0115   |\n",
      "|    n_updates        | 33       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8960, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.964    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 8960     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0106   |\n",
      "|    n_updates        | 34       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9408, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.963    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 9408     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0128   |\n",
      "|    n_updates        | 36       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9856, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.961    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 9856     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 38       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10304, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.959    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10304    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0108   |\n",
      "|    n_updates        | 40       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10752, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.957    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10752    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0115   |\n",
      "|    n_updates        | 41       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11200, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.955    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 11200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0128   |\n",
      "|    n_updates        | 43       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11648, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.954    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 11648    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0122   |\n",
      "|    n_updates        | 45       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12096, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.952    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 12096    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0173   |\n",
      "|    n_updates        | 47       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12544, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.95     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 12544    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 48       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12992, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.948    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 12992    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0108   |\n",
      "|    n_updates        | 50       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13440, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.946    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 13440    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0147   |\n",
      "|    n_updates        | 52       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13888, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.945    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 13888    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00987  |\n",
      "|    n_updates        | 54       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14336, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.943    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 14336    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 55       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14784, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.941    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 14784    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00786  |\n",
      "|    n_updates        | 57       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15232, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.939    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15232    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0146   |\n",
      "|    n_updates        | 59       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15680, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.938    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15680    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00932  |\n",
      "|    n_updates        | 61       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16128, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.936    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 16128    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0103   |\n",
      "|    n_updates        | 62       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16576, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.934    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 16576    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0108   |\n",
      "|    n_updates        | 64       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17024, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.932    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 17024    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0104   |\n",
      "|    n_updates        | 66       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17472, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.93     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 17472    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00853  |\n",
      "|    n_updates        | 68       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17920, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.929    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 17920    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0091   |\n",
      "|    n_updates        | 69       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.928    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 979      |\n",
      "|    total_timesteps  | 17920    |\n",
      "----------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 8     |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 12    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 16    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 20    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 24    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 28    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 32    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 36    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 40    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 44    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 48    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 52    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 56    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 60    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 64    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 979   |\n",
      "|    total_timesteps | 17920 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18368, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.927    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18368    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00843  |\n",
      "|    n_updates        | 71       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18816, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.925    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18816    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0066   |\n",
      "|    n_updates        | 73       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19264, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.923    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 19264    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00732  |\n",
      "|    n_updates        | 75       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19712, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.921    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 19712    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0071   |\n",
      "|    n_updates        | 76       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20160, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.92     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20160    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0115   |\n",
      "|    n_updates        | 78       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20608, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.918    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20608    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00841  |\n",
      "|    n_updates        | 80       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=21056, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.916    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 21056    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0107   |\n",
      "|    n_updates        | 82       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=21504, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.914    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 21504    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0106   |\n",
      "|    n_updates        | 83       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=21952, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.912    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 21952    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0103   |\n",
      "|    n_updates        | 85       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22400, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.911    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 22400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00811  |\n",
      "|    n_updates        | 87       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22848, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.909    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 22848    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0057   |\n",
      "|    n_updates        | 89       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23296, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.907    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 23296    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00585  |\n",
      "|    n_updates        | 90       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23744, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 23744    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00598  |\n",
      "|    n_updates        | 92       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24192, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.903    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 24192    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00667  |\n",
      "|    n_updates        | 94       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24640, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.902    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 24640    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00691  |\n",
      "|    n_updates        | 96       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25088, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.9      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25088    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00905  |\n",
      "|    n_updates        | 97       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25536, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.898    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25536    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00589  |\n",
      "|    n_updates        | 99       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25984, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.896    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25984    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00497  |\n",
      "|    n_updates        | 101      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26432, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.895    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 26432    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00705  |\n",
      "|    n_updates        | 103      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26880, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.893    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 26880    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00527  |\n",
      "|    n_updates        | 104      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27328, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.891    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 27328    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00445  |\n",
      "|    n_updates        | 106      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27776, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.889    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 27776    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0057   |\n",
      "|    n_updates        | 108      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28224, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.887    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 28224    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00239  |\n",
      "|    n_updates        | 110      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28672, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.886    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 28672    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00408  |\n",
      "|    n_updates        | 111      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=29120, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.884    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 29120    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00302  |\n",
      "|    n_updates        | 113      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=29568, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.882    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 29568    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00381  |\n",
      "|    n_updates        | 115      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30016, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.88     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30016    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00657  |\n",
      "|    n_updates        | 117      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30464, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.878    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30464    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00691  |\n",
      "|    n_updates        | 118      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30912, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.877    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30912    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0093   |\n",
      "|    n_updates        | 120      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31360, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.875    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 31360    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00501  |\n",
      "|    n_updates        | 122      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31808, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.873    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 31808    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0061   |\n",
      "|    n_updates        | 124      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32256, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.871    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 32256    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00561  |\n",
      "|    n_updates        | 125      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32704, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.869    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 32704    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0057   |\n",
      "|    n_updates        | 127      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33152, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.868    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 33152    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0044   |\n",
      "|    n_updates        | 129      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33600, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.866    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 33600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00553  |\n",
      "|    n_updates        | 131      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34048, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.864    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 34048    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00337  |\n",
      "|    n_updates        | 132      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34496, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.862    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 34496    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00752  |\n",
      "|    n_updates        | 134      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34944, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.86     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 34944    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00325  |\n",
      "|    n_updates        | 136      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35392, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.859    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 35392    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00327  |\n",
      "|    n_updates        | 138      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35840, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.857    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 35840    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00433  |\n",
      "|    n_updates        | 139      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.857    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 1926     |\n",
      "|    total_timesteps  | 35840    |\n",
      "----------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 72    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 76    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 80    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 84    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 88    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 92    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 96    |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 100   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 104   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 108   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 112   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 116   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 120   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 124   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 128   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 1926  |\n",
      "|    total_timesteps | 35840 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36288, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.855    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 36288    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00361  |\n",
      "|    n_updates        | 141      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36736, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.853    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 36736    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00181  |\n",
      "|    n_updates        | 143      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=37184, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.852    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 37184    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00301  |\n",
      "|    n_updates        | 145      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=37632, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.85     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 37632    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00291  |\n",
      "|    n_updates        | 146      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38080, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.848    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 38080    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00427  |\n",
      "|    n_updates        | 148      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38528, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.846    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 38528    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00357  |\n",
      "|    n_updates        | 150      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38976, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.844    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 38976    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00311  |\n",
      "|    n_updates        | 152      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39424, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.843    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 39424    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00393  |\n",
      "|    n_updates        | 153      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39872, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.841    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 39872    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0025   |\n",
      "|    n_updates        | 155      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40320, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.839    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40320    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00411  |\n",
      "|    n_updates        | 157      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40768, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.837    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40768    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00555  |\n",
      "|    n_updates        | 159      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=41216, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.835    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 41216    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00463  |\n",
      "|    n_updates        | 160      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=41664, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.834    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 41664    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0055   |\n",
      "|    n_updates        | 162      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=42112, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.832    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 42112    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0034   |\n",
      "|    n_updates        | 164      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=42560, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.83     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 42560    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00285  |\n",
      "|    n_updates        | 166      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43008, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.828    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 43008    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00358  |\n",
      "|    n_updates        | 167      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43456, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.826    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 43456    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0026   |\n",
      "|    n_updates        | 169      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43904, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.825    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 43904    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00166  |\n",
      "|    n_updates        | 171      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=44352, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.823    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 44352    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00204  |\n",
      "|    n_updates        | 173      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=44800, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.821    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 44800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00335  |\n",
      "|    n_updates        | 174      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45248, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.819    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 45248    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00288  |\n",
      "|    n_updates        | 176      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45696, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.817    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 45696    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00371  |\n",
      "|    n_updates        | 178      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=46144, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.816    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 46144    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00224  |\n",
      "|    n_updates        | 180      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=46592, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.814    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 46592    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0025   |\n",
      "|    n_updates        | 181      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47040, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.812    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 47040    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00167  |\n",
      "|    n_updates        | 183      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47488, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 47488    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00167  |\n",
      "|    n_updates        | 185      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47936, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.809    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 47936    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00183  |\n",
      "|    n_updates        | 187      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48384, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.807    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 48384    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00111  |\n",
      "|    n_updates        | 188      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48832, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.805    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 48832    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00236  |\n",
      "|    n_updates        | 190      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=49280, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.803    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 49280    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00214  |\n",
      "|    n_updates        | 192      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=49728, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.801    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 49728    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00152  |\n",
      "|    n_updates        | 194      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50176, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.8      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50176    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00255  |\n",
      "|    n_updates        | 195      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50624, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.798    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50624    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00222  |\n",
      "|    n_updates        | 197      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51072, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.796    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 51072    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00177  |\n",
      "|    n_updates        | 199      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51520, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.794    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 51520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00199  |\n",
      "|    n_updates        | 201      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51968, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.792    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 51968    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00155  |\n",
      "|    n_updates        | 202      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=52416, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.791    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 52416    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00132  |\n",
      "|    n_updates        | 204      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=52864, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.789    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 52864    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00215  |\n",
      "|    n_updates        | 206      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=53312, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.787    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 53312    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00124  |\n",
      "|    n_updates        | 208      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=53760, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.785    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 53760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00162  |\n",
      "|    n_updates        | 209      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.785    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 2943     |\n",
      "|    total_timesteps  | 53760    |\n",
      "----------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 136   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 140   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 144   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 148   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 152   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 156   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 160   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 164   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 168   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 172   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 176   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 180   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 184   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 188   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 192   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 2943  |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54208, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.783    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 54208    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00111  |\n",
      "|    n_updates        | 211      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=54656, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.782    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 54656    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00124  |\n",
      "|    n_updates        | 213      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55104, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.78     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 55104    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000587 |\n",
      "|    n_updates        | 215      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55552, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.778    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 55552    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000936 |\n",
      "|    n_updates        | 216      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.776    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 56000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000874 |\n",
      "|    n_updates        | 218      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=56448, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.774    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 56448    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000936 |\n",
      "|    n_updates        | 220      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=56896, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.773    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 56896    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000672 |\n",
      "|    n_updates        | 222      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=57344, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.771    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 57344    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000584 |\n",
      "|    n_updates        | 223      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=57792, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.769    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 57792    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000632 |\n",
      "|    n_updates        | 225      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=58240, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.767    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 58240    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000499 |\n",
      "|    n_updates        | 227      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=58688, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.766    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 58688    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000524 |\n",
      "|    n_updates        | 229      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=59136, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.764    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 59136    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000755 |\n",
      "|    n_updates        | 230      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=59584, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.762    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 59584    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000272 |\n",
      "|    n_updates        | 232      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60032, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.76     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60032    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00112  |\n",
      "|    n_updates        | 234      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.758    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60480    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000879 |\n",
      "|    n_updates        | 236      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60928, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.757    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60928    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000789 |\n",
      "|    n_updates        | 237      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=61376, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.755    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 61376    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000673 |\n",
      "|    n_updates        | 239      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=61824, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.753    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 61824    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00093  |\n",
      "|    n_updates        | 241      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=62272, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.751    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 62272    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000264 |\n",
      "|    n_updates        | 243      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=62720, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.749    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 62720    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000592 |\n",
      "|    n_updates        | 244      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=63168, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.748    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 63168    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000579 |\n",
      "|    n_updates        | 246      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=63616, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.746    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 63616    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000321 |\n",
      "|    n_updates        | 248      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=64064, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.744    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 64064    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000367 |\n",
      "|    n_updates        | 250      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=64512, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.742    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 64512    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00051  |\n",
      "|    n_updates        | 251      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=64960, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.74     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 64960    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000366 |\n",
      "|    n_updates        | 253      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65408, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.739    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 65408    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000547 |\n",
      "|    n_updates        | 255      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65856, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.737    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 65856    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000354 |\n",
      "|    n_updates        | 257      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66304, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.735    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 66304    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0003   |\n",
      "|    n_updates        | 258      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66752, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.733    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 66752    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000157 |\n",
      "|    n_updates        | 260      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=67200, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.731    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 67200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000199 |\n",
      "|    n_updates        | 262      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=67648, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.73     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 67648    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000196 |\n",
      "|    n_updates        | 264      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=68096, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.728    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 68096    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000101 |\n",
      "|    n_updates        | 265      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=68544, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.726    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 68544    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0003   |\n",
      "|    n_updates        | 267      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=68992, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.724    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 68992    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.45e-05 |\n",
      "|    n_updates        | 269      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=69440, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.722    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 69440    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.04e-05 |\n",
      "|    n_updates        | 271      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=69888, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.721    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 69888    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000332 |\n",
      "|    n_updates        | 272      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70336, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.719    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70336    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000211 |\n",
      "|    n_updates        | 274      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70784, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.717    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70784    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000351 |\n",
      "|    n_updates        | 276      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=71232, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.715    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 71232    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000502 |\n",
      "|    n_updates        | 278      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=71680, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.714    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 71680    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000168 |\n",
      "|    n_updates        | 279      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.713    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 3954     |\n",
      "|    total_timesteps  | 71680    |\n",
      "----------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 200   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 204   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 208   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 212   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 216   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 220   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 224   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 228   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 232   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 236   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 240   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 244   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 248   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 252   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 256   |\n",
      "|    fps             | 18    |\n",
      "|    time_elapsed    | 3954  |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72128, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.712    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 72128    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000205 |\n",
      "|    n_updates        | 281      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=72576, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.71     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 72576    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000471 |\n",
      "|    n_updates        | 283      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=73024, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.708    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 73024    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000331 |\n",
      "|    n_updates        | 285      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=73472, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.706    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 73472    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000353 |\n",
      "|    n_updates        | 286      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=73920, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.705    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 73920    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000451 |\n",
      "|    n_updates        | 288      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=74368, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.703    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 74368    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000507 |\n",
      "|    n_updates        | 290      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=74816, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.701    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 74816    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000403 |\n",
      "|    n_updates        | 292      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75264, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.699    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 75264    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00011  |\n",
      "|    n_updates        | 293      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75712, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.697    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 75712    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.93e-05 |\n",
      "|    n_updates        | 295      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=76160, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.696    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 76160    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000149 |\n",
      "|    n_updates        | 297      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=76608, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.694    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 76608    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000129 |\n",
      "|    n_updates        | 299      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=77056, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.692    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 77056    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000267 |\n",
      "|    n_updates        | 300      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=77504, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.69     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 77504    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000238 |\n",
      "|    n_updates        | 302      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=77952, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.688    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 77952    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.18e-05 |\n",
      "|    n_updates        | 304      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=78400, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.687    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 78400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00016  |\n",
      "|    n_updates        | 306      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=78848, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.685    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 78848    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.74e-05 |\n",
      "|    n_updates        | 307      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=79296, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.683    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 79296    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00051  |\n",
      "|    n_updates        | 309      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=79744, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.681    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 79744    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.37e-05 |\n",
      "|    n_updates        | 311      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80192, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.679    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80192    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000244 |\n",
      "|    n_updates        | 313      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80640, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.678    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80640    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000456 |\n",
      "|    n_updates        | 314      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=81088, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.676    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 81088    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.45e-05 |\n",
      "|    n_updates        | 316      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=81536, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.674    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 81536    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000166 |\n",
      "|    n_updates        | 318      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=81984, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.672    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 81984    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000112 |\n",
      "|    n_updates        | 320      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=82432, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.671    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 82432    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000235 |\n",
      "|    n_updates        | 321      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=82880, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.669    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 82880    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000199 |\n",
      "|    n_updates        | 323      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=83328, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.667    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 83328    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000113 |\n",
      "|    n_updates        | 325      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=83776, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.665    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 83776    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.07e-05 |\n",
      "|    n_updates        | 327      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=84224, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.663    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 84224    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.64e-05 |\n",
      "|    n_updates        | 328      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=84672, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.662    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 84672    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000199 |\n",
      "|    n_updates        | 330      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85120, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.66     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 85120    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000191 |\n",
      "|    n_updates        | 332      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85568, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.658    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 85568    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000113 |\n",
      "|    n_updates        | 334      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=86016, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.656    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 86016    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.49e-05 |\n",
      "|    n_updates        | 335      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=86464, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.654    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 86464    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000186 |\n",
      "|    n_updates        | 337      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=86912, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.653    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 86912    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.16e-05 |\n",
      "|    n_updates        | 339      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87360, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.651    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 87360    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.67e-05 |\n",
      "|    n_updates        | 341      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87808, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.649    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 87808    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000233 |\n",
      "|    n_updates        | 342      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88256, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.647    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 88256    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000121 |\n",
      "|    n_updates        | 344      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88704, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.645    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 88704    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.53e-05 |\n",
      "|    n_updates        | 346      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=89152, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.644    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 89152    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000363 |\n",
      "|    n_updates        | 348      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=89600, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.642    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 89600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000449 |\n",
      "|    n_updates        | 349      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.642    |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 17       |\n",
      "|    time_elapsed     | 5024     |\n",
      "|    total_timesteps  | 89600    |\n",
      "----------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 264   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 268   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 272   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 276   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 280   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 284   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 288   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 292   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 296   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 300   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 304   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 308   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 312   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 316   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 320   |\n",
      "|    fps             | 17    |\n",
      "|    time_elapsed    | 5024  |\n",
      "|    total_timesteps | 89600 |\n",
      "------------------------------\n",
      "Eval num_timesteps=90048, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.64     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90048    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000286 |\n",
      "|    n_updates        | 351      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90496, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.638    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90496    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000211 |\n",
      "|    n_updates        | 353      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90944, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.636    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90944    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.69e-05 |\n",
      "|    n_updates        | 355      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=91392, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.635    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 91392    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000199 |\n",
      "|    n_updates        | 356      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=91840, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.633    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 91840    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000249 |\n",
      "|    n_updates        | 358      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=92288, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.631    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 92288    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00017  |\n",
      "|    n_updates        | 360      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=92736, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.629    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 92736    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000144 |\n",
      "|    n_updates        | 362      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=93184, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.628    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 93184    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000187 |\n",
      "|    n_updates        | 363      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=93632, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.626    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 93632    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000153 |\n",
      "|    n_updates        | 365      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=94080, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.624    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 94080    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.99e-05 |\n",
      "|    n_updates        | 367      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=94528, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.622    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 94528    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000431 |\n",
      "|    n_updates        | 369      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=94976, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 94976    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.86e-05 |\n",
      "|    n_updates        | 370      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95424, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.619    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 95424    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000259 |\n",
      "|    n_updates        | 372      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95872, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.617    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 95872    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000324 |\n",
      "|    n_updates        | 374      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=96320, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.615    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 96320    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000127 |\n",
      "|    n_updates        | 376      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=96768, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.613    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 96768    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00011  |\n",
      "|    n_updates        | 377      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=97216, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.611    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 97216    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00015  |\n",
      "|    n_updates        | 379      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=97664, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.61     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 97664    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000107 |\n",
      "|    n_updates        | 381      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=98112, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.608    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 98112    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.65e-05 |\n",
      "|    n_updates        | 383      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=98560, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.606    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 98560    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.12e-05 |\n",
      "|    n_updates        | 384      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99008, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.604    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 99008    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.29e-05 |\n",
      "|    n_updates        | 386      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99456, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.602    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 99456    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000126 |\n",
      "|    n_updates        | 388      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99904, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.601    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 99904    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000255 |\n",
      "|    n_updates        | 390      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100352, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.599    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100352   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000101 |\n",
      "|    n_updates        | 391      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.597    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100800   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.6e-05  |\n",
      "|    n_updates        | 393      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=101248, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.595    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 101248   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000108 |\n",
      "|    n_updates        | 395      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=101696, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.593    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 101696   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000141 |\n",
      "|    n_updates        | 397      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=102144, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.592    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 102144   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000174 |\n",
      "|    n_updates        | 398      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=102592, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.59     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 102592   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000353 |\n",
      "|    n_updates        | 400      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=103040, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 280      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.588    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 103040   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.98e-05 |\n",
      "|    n_updates        | 402      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[1;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 59\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[0;32mIn[59], line 21\u001b[0m, in \u001b[0;36mFlattenDictWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     20\u001b[0m     transformed_action \u001b[38;5;241m=\u001b[39m transform_action_to_int(action)\n\u001b[0;32m---> 21\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     obs \u001b[38;5;241m=\u001b[39m flatten_observation(obs)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium_robotics/envs/franka_kitchen/kitchen_env.py:400\u001b[0m, in \u001b[0;36mKitchenEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 400\u001b[0m     robot_obs, _, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrobot_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs(robot_obs)\n\u001b[1;32m    403\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_reward(obs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124machieved_goal\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal, info)\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium_robotics/envs/franka_kitchen/franka_env.py:103\u001b[0m, in \u001b[0;36mFrankaRobot.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# enforce position limits\u001b[39;00m\n\u001b[1;32m    101\u001b[0m ctrl_feasible \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctrl_position_limits(ctrl_feasible)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctrl_feasible\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_skip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_env.py:200\u001b[0m, in \u001b[0;36mMujocoEnv.do_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(ctrl)\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnu,):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction dimension mismatch. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnu,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39marray(ctrl)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_mujoco_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_env.py:148\u001b[0m, in \u001b[0;36mMujocoEnv._step_mujoco_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03mStep over the MuJoCo simulation.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mctrl[:] \u001b[38;5;241m=\u001b[39m ctrl\n\u001b[0;32m--> 148\u001b[0m \u001b[43mmujoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmj_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# As of MuJoCo 2.0, force-related quantities like cacc are not computed\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# unless there's a force sensor in the model.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# See https://github.com/openai/gym/issues/1541\u001b[39;00m\n\u001b[1;32m    153\u001b[0m mujoco\u001b[38;5;241m.\u001b[39mmj_rnePostConstraint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.learn(total_timesteps=max_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c95743a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "70579843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n",
      "Episode reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\tenv_eval = make_env()\n",
    "\tobs, _ = env_eval.reset()\n",
    "\tdone = False\n",
    "\tep_reward = 0\n",
    "\n",
    "\twhile not done:\n",
    "\t\taction, _ = model.predict(obs, deterministic=True)\n",
    "\t\tobs, reward, terminated, truncated, _ = env_eval.step(transform_action_from_int(action))\n",
    "\t\tobs = flatten_observation(obs)\n",
    "\t\tdone = terminated or truncated\n",
    "\t\tep_reward += reward\n",
    "\tprint(f\"Episode reward: {ep_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
