{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41a0c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from stable_baselines3 import SAC\n",
    "# from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.buffers import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f8a571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'FrankaKitchen-v1'\n",
    "task = 'kettle'\n",
    "gym.register_envs(gymnasium_robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2769def",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b2c4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cb8feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_observation(observation):\n",
    "    if not isinstance(observation, dict):\n",
    "        return observation\n",
    "    achieved = observation['achieved_goal'][task].astype(np.float32)\n",
    "    obs = observation['observation'].astype(np.float32)\n",
    "\n",
    "    flat_obs = np.concatenate([achieved, obs], dtype=np.float32)\n",
    "    return flat_obs\n",
    "\n",
    "def custom_reward(observation):\n",
    "    achieved = observation['achieved_goal'][task][0:4]\n",
    "    desired = observation['desired_goal'][task][0:4]\n",
    "    res = 1.0 - np.linalg.norm(achieved - desired)\n",
    "    # assert res <= 1.0 and res >= 0.0, \"Reward out of range! \"+str(res)\n",
    "    return res   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "253230ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_dim = 59 + 7\n",
    "obs_low = np.full((flat_dim,), -1e10, dtype=np.float32)\n",
    "obs_high = np.full((flat_dim,), 1e10, dtype=np.float32)\n",
    "\n",
    "class FlattenDictWrapper(gym.Wrapper):    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.keys = env.observation_space.spaces.keys()\n",
    "        self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, shape=(flat_dim,), dtype=np.float32)\n",
    "\n",
    "    # def observation(self, observation):\n",
    "    #     return flatten_observation(observation)\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        if reward == 0.0:\n",
    "            reward = custom_reward(obs)\n",
    "        else:\n",
    "            reward = 1000\n",
    "        obs = flatten_observation(obs)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        return flatten_observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93560185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make(env_id, render_mode=None, tasks_to_complete=[task])  # Or your actual task\n",
    "    env = FlattenDictWrapper(env)\n",
    "    return env\n",
    "\n",
    "n_training_envs = 1\n",
    "env = DummyVecEnv([make_env]*n_training_envs)\n",
    "eval_env = DummyVecEnv([make_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41180dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "combos1 = torch.load(\"combos/combos-lr001-hs0-just-success-10000\")\n",
    "combos2 = torch.load(\"combos/combos-lr001-hs0-just-success-10000-5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_buffer(replay_buffer, combos):\n",
    "    for combo in tqdm.tqdm(combos):\n",
    "        state = eval_env.reset()\n",
    "        for combo_step in combo:\n",
    "            action = np.asarray([combo_step])\n",
    "            obs, reward, done, info = eval_env.step(action)\n",
    "            replay_buffer.add(state[0], obs[0], action, reward, done, info)\n",
    "            state = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC(\"MlpPolicy\", env, device=device, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db8869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1209/1209 [12:14<00:00,  1.65it/s]\n",
      "100%|██████████| 636/636 [06:18<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "replay_buffer = model.replay_buffer\n",
    "initialize_buffer(replay_buffer, combos1)\n",
    "initialize_buffer(replay_buffer, combos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17501d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timesteps = 150000\n",
    "run_name = f\"sac_{max_timesteps}_reward_shaping_\"+task\n",
    "eval_log_dir = os.path.join(\"eval_logs\", run_name)\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=eval_log_dir,\n",
    "                              log_path=eval_log_dir, eval_freq=max(500 // n_training_envs, 1),\n",
    "                              n_eval_episodes=5, deterministic=True,\n",
    "                              render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f00759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=1500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=2000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=2500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=3500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=5500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=6500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=7500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=8500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=167.81 +/- 0.05\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=10500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=11500, episode_reward=168.12 +/- 0.06\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=12500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=14500, episode_reward=171.07 +/- 0.71\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=15500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=16500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=17500, episode_reward=168.88 +/- 0.10\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=18500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=19500, episode_reward=165.04 +/- 0.15\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=20500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=21500, episode_reward=167.58 +/- 0.07\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=174.51 +/- 0.68\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=177.18 +/- 1.26\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23500, episode_reward=171.60 +/- 2.88\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=174.51 +/- 0.27\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=24500, episode_reward=180.97 +/- 3.38\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=177.70 +/- 4.28\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=25500, episode_reward=177.14 +/- 0.70\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=179.52 +/- 4.54\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=26500, episode_reward=180.99 +/- 0.69\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=173.22 +/- 2.27\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=27500, episode_reward=175.43 +/- 1.43\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=180.28 +/- 2.97\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=28500, episode_reward=166.27 +/- 8.72\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=145.14 +/- 66.12\n",
      "Episode length: 228.00 +/- 104.00\n",
      "Eval num_timesteps=29500, episode_reward=180.13 +/- 3.87\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=174.13 +/- 5.79\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=30500, episode_reward=172.70 +/- 1.95\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=173.82 +/- 2.36\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=31500, episode_reward=184.10 +/- 7.59\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=182.92 +/- 3.45\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=32500, episode_reward=181.28 +/- 4.57\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=33500, episode_reward=180.96 +/- 0.93\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=167.98 +/- 5.51\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=34500, episode_reward=176.52 +/- 1.69\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=175.87 +/- 2.94\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=35500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=36500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=154.15 +/- 67.98\n",
      "Episode length: 229.60 +/- 100.80\n",
      "Eval num_timesteps=37500, episode_reward=183.20 +/- 6.01\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=172.32 +/- 0.31\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=38500, episode_reward=188.65 +/- 5.67\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=39000, episode_reward=170.98 +/- 10.75\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=39500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=112.27 +/- 76.01\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=40500, episode_reward=168.53 +/- 0.41\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=172.04 +/- 3.02\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=41500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=42500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=43500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=44500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=45500, episode_reward=165.36 +/- 3.60\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=46500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=47500, episode_reward=186.45 +/- 6.94\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=48500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=109.19 +/- 118.05\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=49500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=50500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=51500, episode_reward=171.84 +/- 0.43\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=52500, episode_reward=168.98 +/- 0.82\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=53500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=166.85 +/- 1.69\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=54500, episode_reward=174.48 +/- 1.75\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=167.58 +/- 0.08\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=55500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=56500, episode_reward=167.55 +/- 0.06\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=168.73 +/- 0.94\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=57500, episode_reward=169.94 +/- 3.11\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=176.20 +/- 2.99\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=58500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=170.20 +/- 0.90\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=59500, episode_reward=170.16 +/- 0.49\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=172.37 +/- 1.79\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=60500, episode_reward=102.14 +/- 67.92\n",
      "Episode length: 226.80 +/- 106.40\n",
      "Eval num_timesteps=61000, episode_reward=178.18 +/- 13.39\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=61500, episode_reward=51.69 +/- 63.27\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=195.60 +/- 6.45\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=62500, episode_reward=185.59 +/- 6.01\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=129.69 +/- 62.97\n",
      "Episode length: 199.20 +/- 98.98\n",
      "Eval num_timesteps=63500, episode_reward=-278.30 +/- 232.05\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=170.97 +/- 1.45\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=64500, episode_reward=173.79 +/- 1.88\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=167.95 +/- 1.16\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=65500, episode_reward=156.49 +/- 60.53\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=4.38 +/- 210.29\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=66500, episode_reward=-409.69 +/- 47.97\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=184.38 +/- 6.26\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=67500, episode_reward=129.79 +/- 54.62\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-448.19 +/- 54.57\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=68500, episode_reward=154.24 +/- 73.87\n",
      "Episode length: 226.00 +/- 108.00\n",
      "Eval num_timesteps=69000, episode_reward=171.37 +/- 4.71\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=69500, episode_reward=2.73 +/- 169.12\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=153.95 +/- 72.34\n",
      "Episode length: 226.80 +/- 106.40\n",
      "Eval num_timesteps=70500, episode_reward=167.88 +/- 0.22\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=182.96 +/- 1.74\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=71500, episode_reward=96.72 +/- 168.51\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=190.28 +/- 2.20\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=72500, episode_reward=177.44 +/- 1.32\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=188.46 +/- 3.30\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=73500, episode_reward=169.30 +/- 5.07\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=128.33 +/- 94.25\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=74500, episode_reward=172.57 +/- 3.44\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=186.26 +/- 6.35\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=75500, episode_reward=182.16 +/- 4.02\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-148.29 +/- 165.21\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=76500, episode_reward=57.41 +/- 255.19\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=152.15 +/- 36.42\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=77500, episode_reward=181.54 +/- 6.90\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-38.63 +/- 199.05\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=78500, episode_reward=175.29 +/- 5.61\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=172.85 +/- 3.14\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=79500, episode_reward=172.58 +/- 4.57\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=80500, episode_reward=153.58 +/- 60.54\n",
      "Episode length: 234.60 +/- 90.80\n",
      "Eval num_timesteps=81000, episode_reward=-48.48 +/- 157.79\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=81500, episode_reward=79.65 +/- 89.50\n",
      "Episode length: 118.00 +/- 132.27\n",
      "Eval num_timesteps=82000, episode_reward=-126.27 +/- 214.81\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=82500, episode_reward=174.91 +/- 1.50\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-274.03 +/- 216.66\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=83500, episode_reward=162.94 +/- 5.28\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=184.18 +/- 3.91\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=84500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=184.03 +/- 2.68\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=85500, episode_reward=187.16 +/- 1.83\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=187.18 +/- 10.29\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=86500, episode_reward=-109.47 +/- 117.19\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=187.19 +/- 4.33\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=87500, episode_reward=174.28 +/- 4.02\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=185.63 +/- 7.94\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=88500, episode_reward=188.46 +/- 1.81\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=128.16 +/- 38.85\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=89500, episode_reward=85.56 +/- 186.30\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=185.61 +/- 11.39\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=90500, episode_reward=195.15 +/- 8.36\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=91500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=191.28 +/- 1.20\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=92500, episode_reward=187.39 +/- 6.45\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=152.64 +/- 69.19\n",
      "Episode length: 228.40 +/- 103.20\n",
      "Eval num_timesteps=93500, episode_reward=185.18 +/- 6.96\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-265.68 +/- 228.65\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=94500, episode_reward=-405.83 +/- 29.24\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-339.42 +/- 32.66\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=95500, episode_reward=189.87 +/- 0.85\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=188.56 +/- 4.94\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=96500, episode_reward=-277.44 +/- 64.17\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=178.59 +/- 26.50\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=97500, episode_reward=164.92 +/- 63.50\n",
      "Episode length: 235.80 +/- 88.40\n",
      "Eval num_timesteps=98000, episode_reward=-269.54 +/- 164.32\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=98500, episode_reward=197.93 +/- 0.83\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=99000, episode_reward=174.66 +/- 28.80\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=99500, episode_reward=195.21 +/- 3.51\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=121.36 +/- 66.36\n",
      "Episode length: 228.40 +/- 103.20\n",
      "Eval num_timesteps=100500, episode_reward=-231.18 +/- 172.62\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=112.38 +/- 32.32\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=101500, episode_reward=-403.65 +/- 47.17\n",
      "Episode length: 280.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=max_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a35fe1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x7f1bcf08cfd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load(f'eval_logs/{run_name}/best_model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "616f8110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 0.5982661565294355\n",
      "Episode reward: 0.5982661565294355\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m      8\u001b[0m \taction, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs_, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m \tobs, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \tobs_ \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m     11\u001b[0m \tdone \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m, in \u001b[0;36mFlattenDictWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 15\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m     17\u001b[0m         reward \u001b[38;5;241m=\u001b[39m custom_reward(obs)\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium_robotics/envs/franka_kitchen/kitchen_env.py:400\u001b[0m, in \u001b[0;36mKitchenEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 400\u001b[0m     robot_obs, _, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrobot_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs(robot_obs)\n\u001b[1;32m    403\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_reward(obs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124machieved_goal\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal, info)\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium_robotics/envs/franka_kitchen/franka_env.py:103\u001b[0m, in \u001b[0;36mFrankaRobot.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# enforce position limits\u001b[39;00m\n\u001b[1;32m    101\u001b[0m ctrl_feasible \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctrl_position_limits(ctrl_feasible)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctrl_feasible\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_skip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_env.py:200\u001b[0m, in \u001b[0;36mMujocoEnv.do_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(ctrl)\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnu,):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction dimension mismatch. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnu,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39marray(ctrl)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_mujoco_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_env.py:148\u001b[0m, in \u001b[0;36mMujocoEnv._step_mujoco_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03mStep over the MuJoCo simulation.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mctrl[:] \u001b[38;5;241m=\u001b[39m ctrl\n\u001b[0;32m--> 148\u001b[0m \u001b[43mmujoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmj_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# As of MuJoCo 2.0, force-related quantities like cacc are not computed\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# unless there's a force sensor in the model.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# See https://github.com/openai/gym/issues/1541\u001b[39;00m\n\u001b[1;32m    153\u001b[0m mujoco\u001b[38;5;241m.\u001b[39mmj_rnePostConstraint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "\tenv_eval = make_env()\n",
    "\tobs_, _ = env_eval.reset()\n",
    "\tdone = False\n",
    "\tep_reward = 0\n",
    "\n",
    "\twhile not done:\n",
    "\t\taction, _ = model.predict(obs_, deterministic=True)\n",
    "\t\tobs, reward, terminated, truncated, _ = env_eval.step(action)\n",
    "\t\tobs_ = obs\n",
    "\t\tdone = terminated or truncated\n",
    "\t\t# Ignore reward shaping for evaluation\n",
    "\t\t# if reward < 1.0:\n",
    "\t\t# \treward = 0.0 \n",
    "\t\tif done:\n",
    "\t\t\tep_reward += reward\n",
    "\tprint(f\"Episode reward: {ep_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Box(-1.0, 1.0, (9,), float64)\n",
      "Model action space: Box(-1.0, 1.0, (9,), float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", env_eval.action_space)\n",
    "print(\"Model action space:\", model.action_space)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
