{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a0c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from stable_baselines3 import SAC\n",
    "# from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.buffers import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8a571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'FrankaKitchen-v1'\n",
    "task = 'kettle'\n",
    "gym.register_envs(gymnasium_robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2769def",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2c4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7cb8feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_observation(observation):\n",
    "    if not isinstance(observation, dict):\n",
    "        return observation\n",
    "    achieved = observation['achieved_goal'][task].astype(np.float32)\n",
    "    obs = observation['observation'].astype(np.float32)\n",
    "\n",
    "    flat_obs = np.concatenate([achieved, obs], dtype=np.float32)\n",
    "    return flat_obs\n",
    "\n",
    "def custom_reward(observation):\n",
    "    achieved = observation['achieved_goal'][task][0:4]\n",
    "    desired = observation['desired_goal'][task][0:4]\n",
    "    res = 1.0 - np.linalg.norm(achieved - desired)\n",
    "    # assert res <= 1.0 and res >= 0.0, \"Reward out of range! \"+str(res)\n",
    "    return res   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "253230ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_dim = 59 + 7\n",
    "obs_low = np.full((flat_dim,), -1e10, dtype=np.float32)\n",
    "obs_high = np.full((flat_dim,), 1e10, dtype=np.float32)\n",
    "\n",
    "class FlattenDictWrapper(gym.Wrapper):    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.keys = env.observation_space.spaces.keys()\n",
    "        self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, shape=(flat_dim,), dtype=np.float32)\n",
    "\n",
    "    # def observation(self, observation):\n",
    "    #     return flatten_observation(observation)\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        if reward == 0.0:\n",
    "            reward = custom_reward(obs)\n",
    "        obs = flatten_observation(obs)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        return flatten_observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "93560185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make(env_id, render_mode=None, tasks_to_complete=[task])  # Or your actual task\n",
    "    env = FlattenDictWrapper(env)\n",
    "    return env\n",
    "\n",
    "n_training_envs = 1\n",
    "env = DummyVecEnv([make_env]*n_training_envs)\n",
    "eval_env = DummyVecEnv([make_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "41180dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "combos1 = torch.load(\"combos/combos-lr001-hs0-just-success-10000\")\n",
    "combos2 = torch.load(\"combos/combos-lr001-hs0-just-success-10000-5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58ab55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_buffer(replay_buffer, combos):\n",
    "    for combo in tqdm.tqdm(combos):\n",
    "        state = eval_env.reset()\n",
    "        for combo_step in combo:\n",
    "            action = np.asarray([combo_step])\n",
    "            obs, reward, done, info = eval_env.step(action)\n",
    "            replay_buffer.add(state[0], obs[0], action, reward, done, info)\n",
    "            state = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7db8869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1209/1209 [23:39<00:00,  1.17s/it]\n",
      "100%|██████████| 636/636 [12:38<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "model = SAC(\"MlpPolicy\", env, device=device, learning_rate=0.001)\n",
    "replay_buffer = model.replay_buffer\n",
    "initialize_buffer(replay_buffer, combos1)\n",
    "initialize_buffer(replay_buffer, combos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "17501d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timesteps = 50000\n",
    "run_name = f\"sac_{max_timesteps}_reward_shaping_\"+task\n",
    "eval_log_dir = os.path.join(\"eval_logs\", run_name)\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=eval_log_dir,\n",
    "                              log_path=eval_log_dir, eval_freq=max(500 // n_training_envs, 1),\n",
    "                              n_eval_episodes=5, deterministic=True,\n",
    "                              render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f00759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/repos/rl/rl-value-based/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=1500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=2000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=2500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=3500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=5500, episode_reward=47.77 +/- 12.32\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=6500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=7500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=8500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=9500, episode_reward=140.14 +/- 28.62\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=56.52 +/- 37.65\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=10500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=149.84 +/- 0.18\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=11500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=12500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=167.67 +/- 0.10\n",
      "Episode length: 280.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13500, episode_reward=167.52 +/- 0.00\n",
      "Episode length: 280.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=max_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aaf5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "616f8110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 166.28406681999147\n",
      "Episode reward: 166.28406681999152\n",
      "Episode reward: 166.28406681999144\n",
      "Episode reward: 166.28406681999152\n",
      "Episode reward: 166.28406681999144\n",
      "Episode reward: 166.28406681999144\n",
      "Episode reward: 166.28406681999144\n",
      "Episode reward: 166.28406681999144\n",
      "Episode reward: 166.28406681999144\n",
      "Episode reward: 166.2840668199915\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\tenv_eval = make_env()\n",
    "\tobs, _ = env_eval.reset()\n",
    "\tdone = False\n",
    "\tep_reward = 0\n",
    "\n",
    "\twhile not done:\n",
    "\t\taction, _ = model.predict(obs, deterministic=True)\n",
    "\t\tobs, reward, terminated, truncated, _ = env_eval.step(action)\n",
    "\t\tdone = terminated or truncated\n",
    "\t\tep_reward += reward\n",
    "\tprint(f\"Episode reward: {ep_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Box(-1.0, 1.0, (9,), float64)\n",
      "Model action space: Box(-1.0, 1.0, (9,), float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", env_eval.action_space)\n",
    "print(\"Model action space:\", model.action_space)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
